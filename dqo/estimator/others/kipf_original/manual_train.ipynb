{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from mscn.util import *\n",
    "from mscn.data import get_train_datasets, load_data, make_dataset\n",
    "from mscn.model import SetConv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize_torch(vals, min_val, max_val):\n",
    "    vals = (vals * (max_val - min_val)) + min_val\n",
    "    return torch.exp(vals)\n",
    "\n",
    "\n",
    "def qerror_loss(preds, targets, min_val, max_val):\n",
    "    qerror = []\n",
    "    preds = unnormalize_torch(preds, min_val, max_val)\n",
    "    targets = unnormalize_torch(targets, min_val, max_val)\n",
    "\n",
    "    for i in range(len(targets)):\n",
    "        if (preds[i] > targets[i]).cpu().data.numpy()[0]:\n",
    "            qerror.append(preds[i] / targets[i])\n",
    "        else:\n",
    "            qerror.append(targets[i] / preds[i])\n",
    "    return torch.mean(torch.cat(qerror))\n",
    "\n",
    "def print_qerror(preds_unnorm, labels_unnorm):\n",
    "    qerror = []\n",
    "    for i in range(len(preds_unnorm)):\n",
    "        if preds_unnorm[i] > float(labels_unnorm[i]):\n",
    "            qerror.append(preds_unnorm[i] / float(labels_unnorm[i]))\n",
    "        else:\n",
    "            qerror.append(float(labels_unnorm[i]) / float(preds_unnorm[i]))\n",
    "\n",
    "    print(\"Median: {}\".format(np.median(qerror)))\n",
    "    print(\"90th percentile: {}\".format(np.percentile(qerror, 90)))\n",
    "    print(\"95th percentile: {}\".format(np.percentile(qerror, 95)))\n",
    "    print(\"99th percentile: {}\".format(np.percentile(qerror, 99)))\n",
    "    print(\"Max: {}\".format(np.max(qerror)))\n",
    "    print(\"Mean: {}\".format(np.mean(qerror)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded queries\n",
      "Loaded bitmaps\n",
      "min log(label): 0.0\n",
      "max log(label): 19.94772801931604\n",
      "Number of training samples: 900\n",
      "Number of validation samples: 100\n",
      "Created TensorDataset for training data\n",
      "Created TensorDataset for validation data\n"
     ]
    }
   ],
   "source": [
    "dicts, column_min_max_vals, min_val, max_val, labels_train, labels_test, max_num_joins, max_num_predicates, train_data, test_data = get_train_datasets(\n",
    "        1000, 1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 5)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_num_joins, max_num_predicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1006])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[99][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2vec, column2vec, op2vec, join2vec = dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_feats = len(table2vec) + 1000\n",
    "predicate_feats = len(column2vec) + len(op2vec) + 1\n",
    "join_feats = len(join2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1006, 13, 6)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_feats , predicate_feats, join_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SetConv(sample_feats, predicate_feats, join_feats, 256)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SetConv(\n",
       "  (sample_mlp1): Linear(in_features=1006, out_features=256, bias=True)\n",
       "  (sample_mlp2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (predicate_mlp1): Linear(in_features=13, out_features=256, bias=True)\n",
       "  (predicate_mlp2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (join_mlp1): Linear(in_features=6, out_features=256, bias=True)\n",
       "  (join_mlp2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (out_mlp1): Linear(in_features=768, out_features=256, bias=True)\n",
       "  (out_mlp2): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_loader = DataLoader(train_data, batch_size=1024)\n",
    "test_data_loader = DataLoader(test_data, batch_size=1024)\n",
    "\n",
    "model.train()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 3.276638984680176\n",
      "Epoch 10, loss: 3.1433732509613037\n",
      "Epoch 20, loss: 3.0158510208129883\n",
      "Epoch 30, loss: 2.9168648719787598\n",
      "Epoch 40, loss: 2.8361403942108154\n",
      "Epoch 50, loss: 2.7466354370117188\n",
      "Epoch 60, loss: 2.705319881439209\n",
      "Epoch 70, loss: 2.6309967041015625\n",
      "Epoch 80, loss: 2.6024208068847656\n",
      "Epoch 90, loss: 2.600196599960327\n"
     ]
    }
   ],
   "source": [
    "cuda = False\n",
    "for epoch in range(100):\n",
    "    loss_total = 0.\n",
    "\n",
    "    for batch_idx, data_batch in enumerate(train_data_loader):\n",
    "\n",
    "        samples, predicates, joins, targets, sample_masks, predicate_masks, join_masks = data_batch\n",
    "\n",
    "        samples, predicates, joins, targets = Variable(samples), Variable(predicates), Variable(joins), Variable(\n",
    "            targets)\n",
    "        sample_masks, predicate_masks, join_masks = Variable(sample_masks), Variable(predicate_masks), Variable(\n",
    "            join_masks)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(samples, predicates, joins, sample_masks, predicate_masks, join_masks)\n",
    "        loss = qerror_loss(outputs, targets.float(), min_val, max_val)\n",
    "        loss_total += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch {}, loss: {}\".format(epoch, loss_total / len(train_data_loader)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "workload_name = 'sample'\n",
    "num_materialized_samples = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time per training sample: 0.06440215640597872\n",
      "Prediction time per validation sample: 0.07446050643920898\n",
      "\n",
      "Q-Error training set:\n",
      "Median: 1.4701089086899124\n",
      "90th percentile: 4.5302793730633715\n",
      "95th percentile: 6.662839525210195\n",
      "99th percentile: 18.0\n",
      "Max: 38.333333333333336\n",
      "Mean: 2.4878464752518052\n",
      "\n",
      "Q-Error validation set:\n",
      "Median: 1.810483919854768\n",
      "90th percentile: 7.734071349329144\n",
      "95th percentile: 16.53554852320675\n",
      "99th percentile: 20.587663636363853\n",
      "Max: 62.266363636363636\n",
      "Mean: 4.103475171588795\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get final training and validation set predictions\n",
    "preds_train, t_total = predict(model, train_data_loader, cuda)\n",
    "print(\"Prediction time per training sample: {}\".format(t_total / len(labels_train) * 1000))\n",
    "\n",
    "preds_test, t_total = predict(model, test_data_loader, cuda)\n",
    "print(\"Prediction time per validation sample: {}\".format(t_total / len(labels_test) * 1000))\n",
    "\n",
    "# Unnormalize\n",
    "preds_train_unnorm = unnormalize_labels(preds_train, min_val, max_val)\n",
    "labels_train_unnorm = unnormalize_labels(labels_train, min_val, max_val)\n",
    "\n",
    "preds_test_unnorm = unnormalize_labels(preds_test, min_val, max_val)\n",
    "labels_test_unnorm = unnormalize_labels(labels_test, min_val, max_val)\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\nQ-Error training set:\")\n",
    "print_qerror(preds_train_unnorm, labels_train_unnorm)\n",
    "\n",
    "print(\"\\nQ-Error validation set:\")\n",
    "print_qerror(preds_test_unnorm, labels_test_unnorm)\n",
    "print(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.6852]),\n",
       " tensor([0.2350]),\n",
       " tensor([0.1759]),\n",
       " tensor([0.7172]),\n",
       " tensor([0.8332]),\n",
       " tensor([0.2502]),\n",
       " tensor([0.8048]),\n",
       " tensor([0.7262]),\n",
       " tensor([0.4365]),\n",
       " tensor([0.8305]),\n",
       " tensor([0.3695]),\n",
       " tensor([0.3982]),\n",
       " tensor([0.8009]),\n",
       " tensor([0.3955]),\n",
       " tensor([0.5152]),\n",
       " tensor([0.8401]),\n",
       " tensor([0.6140]),\n",
       " tensor([0.7636]),\n",
       " tensor([0.6170]),\n",
       " tensor([0.7109]),\n",
       " tensor([0.2741]),\n",
       " tensor([0.8982]),\n",
       " tensor([0.6471]),\n",
       " tensor([0.1589]),\n",
       " tensor([0.7095]),\n",
       " tensor([0.8086]),\n",
       " tensor([0.7880]),\n",
       " tensor([0.3298]),\n",
       " tensor([0.6090]),\n",
       " tensor([0.1609]),\n",
       " tensor([0.3511]),\n",
       " tensor([0.8275]),\n",
       " tensor([0.8391]),\n",
       " tensor([0.8752]),\n",
       " tensor([0.5225]),\n",
       " tensor([0.7655]),\n",
       " tensor([0.6385]),\n",
       " tensor([0.8481]),\n",
       " tensor([0.6294]),\n",
       " tensor([0.3185]),\n",
       " tensor([0.1568]),\n",
       " tensor([0.6091]),\n",
       " tensor([0.6061]),\n",
       " tensor([0.7913]),\n",
       " tensor([0.8101]),\n",
       " tensor([0.7488]),\n",
       " tensor([0.6900]),\n",
       " tensor([0.6559]),\n",
       " tensor([0.7834]),\n",
       " tensor([0.2878]),\n",
       " tensor([0.2534]),\n",
       " tensor([0.6450]),\n",
       " tensor([0.7940]),\n",
       " tensor([0.8739]),\n",
       " tensor([0.5446]),\n",
       " tensor([0.1453]),\n",
       " tensor([0.5620]),\n",
       " tensor([0.2499]),\n",
       " tensor([0.6642]),\n",
       " tensor([0.7861]),\n",
       " tensor([0.6636]),\n",
       " tensor([0.7575]),\n",
       " tensor([0.7765]),\n",
       " tensor([0.8222]),\n",
       " tensor([0.3899]),\n",
       " tensor([0.2962]),\n",
       " tensor([0.6727]),\n",
       " tensor([0.2809]),\n",
       " tensor([0.7075]),\n",
       " tensor([0.7313]),\n",
       " tensor([0.8350]),\n",
       " tensor([0.2838]),\n",
       " tensor([0.4702]),\n",
       " tensor([0.6165]),\n",
       " tensor([0.7313]),\n",
       " tensor([0.7095]),\n",
       " tensor([0.7540]),\n",
       " tensor([0.8014]),\n",
       " tensor([0.8041]),\n",
       " tensor([0.7781]),\n",
       " tensor([0.1061]),\n",
       " tensor([0.7314]),\n",
       " tensor([0.5408]),\n",
       " tensor([0.6660]),\n",
       " tensor([0.1370]),\n",
       " tensor([0.7290]),\n",
       " tensor([0.7747]),\n",
       " tensor([0.2906]),\n",
       " tensor([0.8437]),\n",
       " tensor([0.1119]),\n",
       " tensor([0.2956]),\n",
       " tensor([0.2922]),\n",
       " tensor([0.6847]),\n",
       " tensor([0.7121]),\n",
       " tensor([0.3190]),\n",
       " tensor([0.7307]),\n",
       " tensor([0.8471]),\n",
       " tensor([0.7283]),\n",
       " tensor([0.6697]),\n",
       " tensor([0.5084])]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.6829955]), '', (tensor([[[0., 0., 0.,  ..., 0., 1., 0.],\n",
       "           [0., 0., 1.,  ..., 1., 1., 1.],\n",
       "           [0., 0., 0.,  ..., 1., 0., 0.]]]),\n",
       "  tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
       "            0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            1.0000, 0.0000, 0.0000, 1.0000, 0.8705],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 1.0000, 0.0000, 0.0000, 0.0550],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
       "            0.0000, 1.0000, 0.0000, 0.0000, 0.0714],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
       "  tensor([[[0., 0., 0., 1., 0., 0.],\n",
       "           [0., 0., 0., 0., 1., 0.]]]),\n",
       "  tensor([0.6830]),\n",
       "  tensor([[[1.],\n",
       "           [1.],\n",
       "           [1.]]]),\n",
       "  tensor([[[1.],\n",
       "           [1.],\n",
       "           [1.],\n",
       "           [1.],\n",
       "           [0.]]]),\n",
       "  tensor([[[1.],\n",
       "           [1.]]])))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_labels = labels_train[-1:]\n",
    "example_data = train_data[-1:]\n",
    "example_labels,'',example_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 7, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-8b6ead63456e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mexample_data_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpreds_example\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Prediction time per test sample: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_total\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-b73bab433c14>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(model, data_loader, cuda)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicate_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 7, got 1)"
     ]
    }
   ],
   "source": [
    "\n",
    "example_data_loader = DataLoader(example_data, batch_size=1)\n",
    "\n",
    "preds_example, t_total = predict(model, example_data_loader, cuda)\n",
    "print(\"Prediction time per test sample: {}\".format(t_total / len(example_labels) * 1000))\n",
    "\n",
    "# Unnormalize\n",
    "preds_example_unnorm = unnormalize_labels(preds_example, min_val, max_val)\n",
    "# Print metrics\n",
    "print(\"\\nQ-Error example:\")\n",
    "print_qerror(preds_example_unnorm, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
