{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import dataset\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/danieldubovski/projects/deep_query_optimization'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./input/encoded_queries.csv', \n",
    "                   converters={\n",
    "                       \"input_joins\": lambda x: np.array(json.loads(x)),\n",
    "                       \"input_predicates\": lambda x: np.array(json.loads(x)),\n",
    "                       \"input_tables\": lambda x: np.array(json.loads(x)),\n",
    "                       \"input_joins_mask\": lambda x: np.array(json.loads(x)),\n",
    "                       \"input_predicates_mask\": lambda x: np.array(json.loads(x)),\n",
    "                       \"input_tables_mask\": lambda x: np.array(json.loads(x)),\n",
    "                   })\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>runtime</th>\n",
       "      <th>input_joins</th>\n",
       "      <th>input_joins_mask</th>\n",
       "      <th>input_predicates</th>\n",
       "      <th>input_predicates_mask</th>\n",
       "      <th>input_tables</th>\n",
       "      <th>input_tables_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7.040452</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0], [0], [0], [0], [0], [0], [0], [0], [0], ...</td>\n",
       "      <td>[[0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1], [0, 1, 0, ...</td>\n",
       "      <td>[[1], [1], [1], [1], [1], [1], [1], [1], [0], ...</td>\n",
       "      <td>[[0, 0, 1, 0, 1], [0, 0, 1, 1, 0], [0, 1, 0, 0...</td>\n",
       "      <td>[[1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7.740461</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0], [0], [0], [0], [0], [0], [0], [0], [0], ...</td>\n",
       "      <td>[[0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1], [1, 0, 1, ...</td>\n",
       "      <td>[[1], [1], [1], [1], [1], [0], [0], [0], [0], ...</td>\n",
       "      <td>[[0, 0, 0, 1, 1], [0, 0, 0, 1, 0], [0, 0, 1, 0...</td>\n",
       "      <td>[[1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.240947</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0], [0], [0], [0], [0], [0], [0], [0], [0], ...</td>\n",
       "      <td>[[0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1], [0, 1, 1, ...</td>\n",
       "      <td>[[1], [1], [1], [1], [1], [0], [0], [0], [0], ...</td>\n",
       "      <td>[[0, 0, 1, 1, 0], [0, 1, 0, 0, 0], [0, 1, 1, 0...</td>\n",
       "      <td>[[1.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>39.013127</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0], [0], [0], [0], [0], [0], [0], [0], [0], ...</td>\n",
       "      <td>[[0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1], [0, 1, 0, ...</td>\n",
       "      <td>[[1], [1], [1], [1], [1], [1], [1], [1], [0], ...</td>\n",
       "      <td>[[0, 0, 1, 1, 1], [0, 0, 1, 0, 0], [0, 0, 1, 0...</td>\n",
       "      <td>[[1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.188286</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0], [0], [0], [0], [0], [0], [0], [0], [0], ...</td>\n",
       "      <td>[[0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1], [0, 1, 1, ...</td>\n",
       "      <td>[[1], [1], [1], [1], [1], [1], [1], [1], [0], ...</td>\n",
       "      <td>[[0, 0, 1, 0, 1], [0, 0, 1, 0, 1], [0, 1, 0, 0...</td>\n",
       "      <td>[[1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   input    runtime                                        input_joins  \\\n",
       "0    NaN   7.040452  [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, ...   \n",
       "1    NaN   7.740461  [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, ...   \n",
       "2    NaN   1.240947  [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, ...   \n",
       "3    NaN  39.013127  [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, ...   \n",
       "4    NaN   0.188286  [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                    input_joins_mask  \\\n",
       "0  [[0], [0], [0], [0], [0], [0], [0], [0], [0], ...   \n",
       "1  [[0], [0], [0], [0], [0], [0], [0], [0], [0], ...   \n",
       "2  [[0], [0], [0], [0], [0], [0], [0], [0], [0], ...   \n",
       "3  [[0], [0], [0], [0], [0], [0], [0], [0], [0], ...   \n",
       "4  [[0], [0], [0], [0], [0], [0], [0], [0], [0], ...   \n",
       "\n",
       "                                    input_predicates  \\\n",
       "0  [[0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1], [0, 1, 0, ...   \n",
       "1  [[0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1], [1, 0, 1, ...   \n",
       "2  [[0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1], [0, 1, 1, ...   \n",
       "3  [[0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1], [0, 1, 0, ...   \n",
       "4  [[0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1], [0, 1, 1, ...   \n",
       "\n",
       "                               input_predicates_mask  \\\n",
       "0  [[1], [1], [1], [1], [1], [1], [1], [1], [0], ...   \n",
       "1  [[1], [1], [1], [1], [1], [0], [0], [0], [0], ...   \n",
       "2  [[1], [1], [1], [1], [1], [0], [0], [0], [0], ...   \n",
       "3  [[1], [1], [1], [1], [1], [1], [1], [1], [0], ...   \n",
       "4  [[1], [1], [1], [1], [1], [1], [1], [1], [0], ...   \n",
       "\n",
       "                                        input_tables  \\\n",
       "0  [[0, 0, 1, 0, 1], [0, 0, 1, 1, 0], [0, 1, 0, 0...   \n",
       "1  [[0, 0, 0, 1, 1], [0, 0, 0, 1, 0], [0, 0, 1, 0...   \n",
       "2  [[0, 0, 1, 1, 0], [0, 1, 0, 0, 0], [0, 1, 1, 0...   \n",
       "3  [[0, 0, 1, 1, 1], [0, 0, 1, 0, 0], [0, 0, 1, 0...   \n",
       "4  [[0, 0, 1, 0, 1], [0, 0, 1, 0, 1], [0, 1, 0, 0...   \n",
       "\n",
       "                                   input_tables_mask  \n",
       "0  [[1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1....  \n",
       "1  [[1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1....  \n",
       "2  [[1.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0....  \n",
       "3  [[1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1....  \n",
       "4  [[1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1....  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "m = np.vstack(data.input_tables.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "m = np.vstack(data.input_tables.apply(lambda v: [int(''.join([str(i) for i in l]),2) for l in v.tolist()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a388cd6d8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21, 5), (210, 8), (108, 11))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.input_tables.iloc[0].shape, data.input_joins.iloc[0].shape,data.input_predicates.iloc[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# PREP DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "tables_tensor = torch.FloatTensor(data.input_tables)\n",
    "tables_mask_tensor = torch.FloatTensor(data.input_tables_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "joins_tensor = torch.FloatTensor(data.input_joins)\n",
    "joins_mask_tensor = torch.FloatTensor(data.input_joins_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "predicates_tensor = torch.FloatTensor(data.input_predicates)\n",
    "predicates_mask_tensor = torch.FloatTensor(data.input_predicates_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1225.000000\n",
       "mean       11.877314\n",
       "std        27.271458\n",
       "min         0.009534\n",
       "25%         0.336270\n",
       "50%         2.549016\n",
       "75%        11.899332\n",
       "max       496.032421\n",
       "Name: runtime, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.runtime.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def normalize_labels(labels, min_val=None, max_val=None):\n",
    "    labels = np.array([np.log(float(l)) for l in labels])\n",
    "    if min_val is None:\n",
    "        min_val = labels.min()\n",
    "        print(\"min log(label): {}\".format(min_val))\n",
    "    if max_val is None:\n",
    "        max_val = labels.max()\n",
    "        print(\"max log(label): {}\".format(max_val))\n",
    "    labels_norm = (labels - min_val) / (max_val - min_val)\n",
    "    # Threshold labels\n",
    "    labels_norm = np.minimum(labels_norm, 1)\n",
    "    labels_norm = np.maximum(labels_norm, 0)\n",
    "    return labels_norm, min_val, max_val\n",
    "\n",
    "\n",
    "def unnormalize_labels(labels_norm, min_val, max_val):\n",
    "    labels_norm = np.array(labels_norm, dtype=np.float32)\n",
    "    labels = (labels_norm * (max_val - min_val)) + min_val\n",
    "    return np.array(np.round(np.exp(labels)), dtype=np.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min log(label): -4.6529032842317255\n",
      "max log(label): 6.206641289253351\n"
     ]
    }
   ],
   "source": [
    "normal_runtime, min_norm,max_norm = normalize_labels(data.runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "target_tensor = torch.FloatTensor(normal_runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1102"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = 0.9\n",
    "rows = len(data)\n",
    "train_size = int(split * rows)\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "labels_train = target_tensor[:train_size]\n",
    "labels_test = target_tensor[train_size:]\n",
    "labels_rsample = target_tensor[-8:-7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'normal: tensor([0.7311]), original: 26.73797082901001'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'normal: {labels_rsample}, original: {data.runtime.iloc[-8]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "train_data = dataset.TensorDataset(\n",
    "    tables_tensor[:train_size], \n",
    "    predicates_tensor[:train_size], \n",
    "    joins_tensor[:train_size], \n",
    "    target_tensor[:train_size], \n",
    "    tables_mask_tensor[:train_size],\n",
    "    predicates_mask_tensor[:train_size], \n",
    "    joins_mask_tensor[:train_size]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "test_data = dataset.TensorDataset(\n",
    "    tables_tensor[train_size:], \n",
    "    predicates_tensor[train_size:], \n",
    "    joins_tensor[train_size:], \n",
    "    target_tensor[train_size:], \n",
    "    tables_mask_tensor[train_size:],\n",
    "    predicates_mask_tensor[train_size:], \n",
    "    joins_mask_tensor[train_size:]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# RUN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "from dqo.estimator.kipf.model import SetConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "tables_shape = data.input_tables.iloc[0].shape\n",
    "joins_shape = data.input_joins.iloc[0].shape \n",
    "predicates_shape = data.input_predicates.iloc[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21, 5), (210, 8), (108, 11))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0 - size of items with padding\n",
    "# 1 - bit to represent data (record size)\n",
    "tables_shape, joins_shape, predicates_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "num_epochs = 200 # 100\n",
    "batch_size = 256 #1024\n",
    "out_hidden = 64 # 256\n",
    "model = SetConv(tables_shape[1], predicates_shape[1], joins_shape[1], 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "train_data_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "test_data_loader = DataLoader(test_data, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SetConv(\n",
       "  (sample_mlp1): Linear(in_features=5, out_features=64, bias=True)\n",
       "  (sample_mlp2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (predicate_mlp1): Linear(in_features=11, out_features=64, bias=True)\n",
       "  (predicate_mlp2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (join_mlp1): Linear(in_features=8, out_features=64, bias=True)\n",
       "  (join_mlp2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (out_mlp1): Linear(in_features=192, out_features=64, bias=True)\n",
       "  (out_mlp2): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SetConv(\n",
       "  (sample_mlp1): Linear(in_features=5, out_features=64, bias=True)\n",
       "  (sample_mlp2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (predicate_mlp1): Linear(in_features=11, out_features=64, bias=True)\n",
       "  (predicate_mlp2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (join_mlp1): Linear(in_features=8, out_features=64, bias=True)\n",
       "  (join_mlp2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (out_mlp1): Linear(in_features=192, out_features=64, bias=True)\n",
       "  (out_mlp2): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def qerror_loss(preds, targets, min_val, max_val):\n",
    "    qerror = []\n",
    "    preds = unnormalize_torch(preds, min_val, max_val)\n",
    "    targets = unnormalize_torch(targets, min_val, max_val)\n",
    "    \n",
    "    for i in range(len(targets)):\n",
    "        if (preds[i] > targets[i]).cpu().data.numpy()[0]:\n",
    "            #loss = preds[i] / targets[i]\n",
    "            loss = (preds[i] - targets[i]) ** 2\n",
    "        else:\n",
    "            #loss = targets[i] / preds[i]\n",
    "            loss = (targets[i] - preds[i]) ** 2\n",
    "        qerror.append(loss)\n",
    "    return torch.mean(torch.cat(qerror))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def unnormalize_torch(vals, min_val, max_val):\n",
    "    vals = (vals * (max_val - min_val)) + min_val\n",
    "    return torch.exp(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def print_qerror(preds_unnorm, labels_unnorm):\n",
    "    qerror = []\n",
    "    for i in range(len(preds_unnorm)):\n",
    "        if preds_unnorm[i] > float(labels_unnorm[i]):\n",
    "            qerror.append(preds_unnorm[i] / float(labels_unnorm[i]))\n",
    "        else:\n",
    "            qerror.append(float(labels_unnorm[i]) / float(preds_unnorm[i]))\n",
    "\n",
    "    print(\"Median: {}\".format(np.median(qerror)))\n",
    "    print(\"90th percentile: {}\".format(np.percentile(qerror, 90)))\n",
    "    print(\"95th percentile: {}\".format(np.percentile(qerror, 95)))\n",
    "    print(\"99th percentile: {}\".format(np.percentile(qerror, 99)))\n",
    "    print(\"Max: {}\".format(np.max(qerror)))\n",
    "    print(\"Mean: {}\".format(np.mean(qerror)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 186.33874130249023\n",
      "Epoch 1, loss: 186.37261199951172\n",
      "Epoch 2, loss: 186.34397506713867\n",
      "Epoch 3, loss: 186.23007583618164\n",
      "Epoch 4, loss: 186.1723976135254\n",
      "Epoch 5, loss: 186.15082931518555\n",
      "Epoch 6, loss: 186.1214256286621\n",
      "Epoch 7, loss: 186.06994247436523\n",
      "Epoch 8, loss: 185.98630905151367\n",
      "Epoch 9, loss: 185.9031524658203\n",
      "Epoch 10, loss: 185.98293685913086\n",
      "Epoch 11, loss: 185.9712371826172\n",
      "Epoch 12, loss: 185.96614837646484\n",
      "Epoch 13, loss: 185.70172882080078\n",
      "Epoch 14, loss: 186.1938362121582\n",
      "Epoch 15, loss: 185.72624588012695\n",
      "Epoch 16, loss: 185.67767333984375\n",
      "Epoch 17, loss: 185.4918212890625\n",
      "Epoch 18, loss: 185.46351623535156\n",
      "Epoch 19, loss: 185.3696746826172\n",
      "Epoch 20, loss: 185.33049774169922\n",
      "Epoch 21, loss: 185.32200622558594\n",
      "Epoch 22, loss: 185.2066421508789\n",
      "Epoch 23, loss: 185.11282348632812\n",
      "Epoch 24, loss: 185.22379302978516\n",
      "Epoch 25, loss: 185.10138320922852\n",
      "Epoch 26, loss: 185.0898208618164\n",
      "Epoch 27, loss: 184.89475631713867\n",
      "Epoch 28, loss: 185.41311264038086\n",
      "Epoch 29, loss: 184.99637603759766\n",
      "Epoch 30, loss: 185.35687637329102\n",
      "Epoch 31, loss: 184.75344848632812\n",
      "Epoch 32, loss: 186.9149627685547\n",
      "Epoch 33, loss: 185.28545761108398\n",
      "Epoch 34, loss: 187.53137969970703\n",
      "Epoch 35, loss: 185.03588485717773\n",
      "Epoch 36, loss: 192.69327926635742\n",
      "Epoch 37, loss: 188.21923446655273\n",
      "Epoch 38, loss: 190.55011749267578\n",
      "Epoch 39, loss: 197.68201065063477\n",
      "Epoch 40, loss: 199.65338134765625\n",
      "Epoch 41, loss: 204.47303771972656\n",
      "Epoch 42, loss: 242.00793838500977\n",
      "Epoch 43, loss: 228.1750717163086\n",
      "Epoch 44, loss: 214.47696685791016\n",
      "Epoch 45, loss: 219.07983779907227\n",
      "Epoch 46, loss: 215.50272750854492\n",
      "Epoch 47, loss: 211.50765228271484\n",
      "Epoch 48, loss: 236.6150665283203\n",
      "Epoch 49, loss: 235.3429718017578\n",
      "Epoch 50, loss: 226.24662017822266\n",
      "Epoch 51, loss: 226.79305267333984\n",
      "Epoch 52, loss: 212.72126007080078\n",
      "Epoch 53, loss: 210.55464553833008\n",
      "Epoch 54, loss: 197.04424285888672\n",
      "Epoch 55, loss: 210.82506942749023\n",
      "Epoch 56, loss: 205.23290634155273\n",
      "Epoch 57, loss: 205.88921737670898\n",
      "Epoch 58, loss: 197.51482391357422\n",
      "Epoch 59, loss: 195.49746322631836\n",
      "Epoch 60, loss: 187.89673614501953\n",
      "Epoch 61, loss: 190.19379425048828\n",
      "Epoch 62, loss: 189.64671325683594\n",
      "Epoch 63, loss: 189.00270462036133\n",
      "Epoch 64, loss: 186.7064666748047\n",
      "Epoch 65, loss: 186.45550155639648\n",
      "Epoch 66, loss: 185.9198760986328\n",
      "Epoch 67, loss: 186.12434768676758\n",
      "Epoch 68, loss: 186.25025939941406\n",
      "Epoch 69, loss: 185.59258651733398\n",
      "Epoch 70, loss: 184.84442138671875\n",
      "Epoch 71, loss: 184.6133575439453\n",
      "Epoch 72, loss: 184.96299362182617\n",
      "Epoch 73, loss: 184.71378326416016\n",
      "Epoch 74, loss: 184.71525192260742\n",
      "Epoch 75, loss: 184.4167366027832\n",
      "Epoch 76, loss: 184.3467788696289\n",
      "Epoch 77, loss: 184.13331985473633\n",
      "Epoch 78, loss: 184.21931838989258\n",
      "Epoch 79, loss: 184.06240463256836\n",
      "Epoch 80, loss: 184.0087127685547\n",
      "Epoch 81, loss: 183.92576217651367\n",
      "Epoch 82, loss: 183.92221069335938\n",
      "Epoch 83, loss: 183.82233047485352\n",
      "Epoch 84, loss: 183.8120574951172\n",
      "Epoch 85, loss: 183.7496223449707\n",
      "Epoch 86, loss: 183.68744277954102\n",
      "Epoch 87, loss: 183.62100982666016\n",
      "Epoch 88, loss: 183.5724639892578\n",
      "Epoch 89, loss: 183.53938674926758\n",
      "Epoch 90, loss: 183.49235916137695\n",
      "Epoch 91, loss: 183.47722625732422\n",
      "Epoch 92, loss: 183.41538619995117\n",
      "Epoch 93, loss: 183.38302612304688\n",
      "Epoch 94, loss: 183.3253059387207\n",
      "Epoch 95, loss: 183.2851905822754\n",
      "Epoch 96, loss: 183.23821258544922\n",
      "Epoch 97, loss: 183.20072555541992\n",
      "Epoch 98, loss: 183.16114044189453\n",
      "Epoch 99, loss: 183.13272094726562\n",
      "Epoch 100, loss: 183.0963363647461\n",
      "Epoch 101, loss: 183.05035018920898\n",
      "Epoch 102, loss: 183.01920700073242\n",
      "Epoch 103, loss: 182.97174835205078\n",
      "Epoch 104, loss: 182.93353271484375\n",
      "Epoch 105, loss: 182.89542388916016\n",
      "Epoch 106, loss: 182.86414337158203\n",
      "Epoch 107, loss: 182.82935333251953\n",
      "Epoch 108, loss: 182.79145431518555\n",
      "Epoch 109, loss: 182.75506210327148\n",
      "Epoch 110, loss: 182.71003341674805\n",
      "Epoch 111, loss: 182.6835060119629\n",
      "Epoch 112, loss: 182.63274002075195\n",
      "Epoch 113, loss: 182.60590362548828\n",
      "Epoch 114, loss: 182.56588745117188\n",
      "Epoch 115, loss: 182.52845764160156\n",
      "Epoch 116, loss: 182.48787307739258\n",
      "Epoch 117, loss: 182.46142578125\n",
      "Epoch 118, loss: 182.4298973083496\n",
      "Epoch 119, loss: 182.40861129760742\n",
      "Epoch 120, loss: 182.36649322509766\n",
      "Epoch 121, loss: 182.31807327270508\n",
      "Epoch 122, loss: 182.26668167114258\n",
      "Epoch 123, loss: 182.2220230102539\n",
      "Epoch 124, loss: 182.19701385498047\n",
      "Epoch 125, loss: 182.1760139465332\n",
      "Epoch 126, loss: 182.15089797973633\n",
      "Epoch 127, loss: 182.10284042358398\n",
      "Epoch 128, loss: 182.05842971801758\n",
      "Epoch 129, loss: 182.0107307434082\n",
      "Epoch 130, loss: 181.9738883972168\n",
      "Epoch 131, loss: 181.93462753295898\n",
      "Epoch 132, loss: 181.90697479248047\n",
      "Epoch 133, loss: 181.86930465698242\n",
      "Epoch 134, loss: 181.83919525146484\n",
      "Epoch 135, loss: 181.79970169067383\n",
      "Epoch 136, loss: 181.76503372192383\n",
      "Epoch 137, loss: 181.72679901123047\n",
      "Epoch 138, loss: 181.68824195861816\n",
      "Epoch 139, loss: 181.64566802978516\n",
      "Epoch 140, loss: 181.60939407348633\n",
      "Epoch 141, loss: 181.57439041137695\n",
      "Epoch 142, loss: 181.55305099487305\n",
      "Epoch 143, loss: 181.5080909729004\n",
      "Epoch 144, loss: 181.47596168518066\n",
      "Epoch 145, loss: 181.4191551208496\n",
      "Epoch 146, loss: 181.40214729309082\n",
      "Epoch 147, loss: 181.361967086792\n",
      "Epoch 148, loss: 181.33460998535156\n",
      "Epoch 149, loss: 181.28980255126953\n",
      "Epoch 150, loss: 181.24703788757324\n",
      "Epoch 151, loss: 181.27885055541992\n",
      "Epoch 152, loss: 181.20282554626465\n",
      "Epoch 153, loss: 181.14515113830566\n",
      "Epoch 154, loss: 181.11421012878418\n",
      "Epoch 155, loss: 181.07348823547363\n",
      "Epoch 156, loss: 181.04980659484863\n",
      "Epoch 157, loss: 181.00202178955078\n",
      "Epoch 158, loss: 180.9763069152832\n",
      "Epoch 159, loss: 180.92283630371094\n",
      "Epoch 160, loss: 180.91314888000488\n",
      "Epoch 161, loss: 180.8648509979248\n",
      "Epoch 162, loss: 180.84168243408203\n",
      "Epoch 163, loss: 180.78132247924805\n",
      "Epoch 164, loss: 180.7685604095459\n",
      "Epoch 165, loss: 180.70281982421875\n",
      "Epoch 166, loss: 180.71154975891113\n",
      "Epoch 167, loss: 180.65649223327637\n",
      "Epoch 168, loss: 180.6214942932129\n",
      "Epoch 169, loss: 180.56522178649902\n",
      "Epoch 170, loss: 180.55083847045898\n",
      "Epoch 171, loss: 180.48419952392578\n",
      "Epoch 172, loss: 180.53432273864746\n",
      "Epoch 173, loss: 180.42785453796387\n",
      "Epoch 174, loss: 180.40548706054688\n",
      "Epoch 175, loss: 180.35087203979492\n",
      "Epoch 176, loss: 180.29572105407715\n",
      "Epoch 177, loss: 180.26343727111816\n",
      "Epoch 178, loss: 180.22193336486816\n",
      "Epoch 179, loss: 180.2007942199707\n",
      "Epoch 180, loss: 180.16305541992188\n",
      "Epoch 181, loss: 180.14282417297363\n",
      "Epoch 182, loss: 180.09974479675293\n",
      "Epoch 183, loss: 180.0822238922119\n",
      "Epoch 184, loss: 180.01372718811035\n",
      "Epoch 185, loss: 180.04651260375977\n",
      "Epoch 186, loss: 179.97331047058105\n",
      "Epoch 187, loss: 179.93315315246582\n",
      "Epoch 188, loss: 179.8929500579834\n",
      "Epoch 189, loss: 179.9143943786621\n",
      "Epoch 190, loss: 179.78426361083984\n",
      "Epoch 191, loss: 179.74591827392578\n",
      "Epoch 192, loss: 179.69571495056152\n",
      "Epoch 193, loss: 179.7117156982422\n",
      "Epoch 194, loss: 179.6599521636963\n",
      "Epoch 195, loss: 179.63120079040527\n",
      "Epoch 196, loss: 179.57598495483398\n",
      "Epoch 197, loss: 179.51984977722168\n",
      "Epoch 198, loss: 179.47071075439453\n",
      "Epoch 199, loss: 179.41215896606445\n",
      "Epoch 200, loss: 179.384614944458\n",
      "Epoch 201, loss: 179.3404941558838\n",
      "Epoch 202, loss: 179.3095359802246\n",
      "Epoch 203, loss: 179.26836395263672\n",
      "Epoch 204, loss: 179.23209190368652\n",
      "Epoch 205, loss: 179.18459701538086\n",
      "Epoch 206, loss: 179.1440887451172\n",
      "Epoch 207, loss: 179.1191463470459\n",
      "Epoch 208, loss: 179.07007789611816\n",
      "Epoch 209, loss: 179.0207862854004\n",
      "Epoch 210, loss: 178.97128105163574\n",
      "Epoch 211, loss: 178.9315357208252\n",
      "Epoch 212, loss: 178.90249252319336\n",
      "Epoch 213, loss: 178.83695220947266\n",
      "Epoch 214, loss: 178.81546783447266\n",
      "Epoch 215, loss: 178.76722145080566\n",
      "Epoch 216, loss: 178.73475456237793\n",
      "Epoch 217, loss: 178.6881217956543\n",
      "Epoch 218, loss: 178.63733863830566\n",
      "Epoch 219, loss: 178.60645866394043\n",
      "Epoch 220, loss: 178.55831146240234\n",
      "Epoch 221, loss: 178.50697135925293\n",
      "Epoch 222, loss: 178.44683265686035\n",
      "Epoch 223, loss: 178.43172645568848\n",
      "Epoch 224, loss: 178.41327667236328\n",
      "Epoch 225, loss: 178.35767364501953\n",
      "Epoch 226, loss: 178.30436515808105\n",
      "Epoch 227, loss: 178.24326133728027\n",
      "Epoch 228, loss: 178.21191596984863\n",
      "Epoch 229, loss: 178.17941665649414\n",
      "Epoch 230, loss: 178.13679695129395\n",
      "Epoch 231, loss: 178.0842227935791\n",
      "Epoch 232, loss: 178.04180908203125\n",
      "Epoch 233, loss: 178.0090503692627\n",
      "Epoch 234, loss: 177.97716903686523\n",
      "Epoch 235, loss: 177.92403984069824\n",
      "Epoch 236, loss: 177.8739185333252\n",
      "Epoch 237, loss: 177.81306838989258\n",
      "Epoch 238, loss: 177.7803726196289\n",
      "Epoch 239, loss: 177.73192977905273\n",
      "Epoch 240, loss: 177.68596649169922\n",
      "Epoch 241, loss: 177.66191482543945\n",
      "Epoch 242, loss: 177.61462593078613\n",
      "Epoch 243, loss: 177.5636043548584\n",
      "Epoch 244, loss: 177.52074813842773\n",
      "Epoch 245, loss: 177.46949577331543\n",
      "Epoch 246, loss: 177.4234848022461\n",
      "Epoch 247, loss: 177.37283325195312\n",
      "Epoch 248, loss: 177.34055519104004\n",
      "Epoch 249, loss: 177.2971954345703\n",
      "Epoch 250, loss: 177.25537490844727\n",
      "Epoch 251, loss: 177.2129077911377\n",
      "Epoch 252, loss: 177.15213012695312\n",
      "Epoch 253, loss: 177.1022186279297\n",
      "Epoch 254, loss: 177.06591415405273\n",
      "Epoch 255, loss: 177.01796340942383\n",
      "Epoch 256, loss: 176.9723720550537\n",
      "Epoch 257, loss: 176.91311645507812\n",
      "Epoch 258, loss: 176.89634323120117\n",
      "Epoch 259, loss: 176.84000778198242\n",
      "Epoch 260, loss: 176.78018760681152\n",
      "Epoch 261, loss: 176.7763557434082\n",
      "Epoch 262, loss: 176.7370948791504\n",
      "Epoch 263, loss: 176.66142654418945\n",
      "Epoch 264, loss: 176.5987319946289\n",
      "Epoch 265, loss: 176.58481216430664\n",
      "Epoch 266, loss: 176.54014205932617\n",
      "Epoch 267, loss: 176.48536682128906\n",
      "Epoch 268, loss: 176.43412590026855\n",
      "Epoch 269, loss: 176.40841102600098\n",
      "Epoch 270, loss: 176.37181282043457\n",
      "Epoch 271, loss: 176.3173885345459\n",
      "Epoch 272, loss: 176.27567291259766\n",
      "Epoch 273, loss: 176.22741889953613\n",
      "Epoch 274, loss: 176.16925048828125\n",
      "Epoch 275, loss: 176.1794147491455\n",
      "Epoch 276, loss: 176.12225341796875\n",
      "Epoch 277, loss: 176.11677742004395\n",
      "Epoch 278, loss: 175.9898624420166\n",
      "Epoch 279, loss: 176.1366729736328\n",
      "Epoch 280, loss: 176.0141830444336\n",
      "Epoch 281, loss: 176.0760669708252\n",
      "Epoch 282, loss: 176.0178985595703\n",
      "Epoch 283, loss: 176.26585388183594\n",
      "Epoch 284, loss: 176.44549179077148\n",
      "Epoch 285, loss: 176.60272216796875\n",
      "Epoch 286, loss: 178.09422302246094\n",
      "Epoch 287, loss: 176.51859855651855\n",
      "Epoch 288, loss: 177.556058883667\n",
      "Epoch 289, loss: 175.89264488220215\n",
      "Epoch 290, loss: 177.9318962097168\n",
      "Epoch 291, loss: 176.2527675628662\n",
      "Epoch 292, loss: 177.6007080078125\n",
      "Epoch 293, loss: 175.84242057800293\n",
      "Epoch 294, loss: 176.07606506347656\n",
      "Epoch 295, loss: 175.69471740722656\n",
      "Epoch 296, loss: 175.481595993042\n",
      "Epoch 297, loss: 175.45982551574707\n",
      "Epoch 298, loss: 175.32778358459473\n",
      "Epoch 299, loss: 175.22992134094238\n",
      "Epoch 300, loss: 175.23145294189453\n",
      "Epoch 301, loss: 175.14345169067383\n",
      "Epoch 302, loss: 175.40094566345215\n",
      "Epoch 303, loss: 175.00917053222656\n",
      "Epoch 304, loss: 175.72854804992676\n",
      "Epoch 305, loss: 175.22497749328613\n",
      "Epoch 306, loss: 176.37324142456055\n",
      "Epoch 307, loss: 175.06068420410156\n",
      "Epoch 308, loss: 177.7475357055664\n",
      "Epoch 309, loss: 176.5729637145996\n",
      "Epoch 310, loss: 179.54838180541992\n",
      "Epoch 311, loss: 180.14484024047852\n",
      "Epoch 312, loss: 176.07335472106934\n",
      "Epoch 313, loss: 176.70255851745605\n",
      "Epoch 314, loss: 174.5803737640381\n",
      "Epoch 315, loss: 174.7833652496338\n",
      "Epoch 316, loss: 174.646484375\n",
      "Epoch 317, loss: 174.54848289489746\n",
      "Epoch 318, loss: 174.4683837890625\n",
      "Epoch 319, loss: 174.38322067260742\n",
      "Epoch 320, loss: 174.4212589263916\n",
      "Epoch 321, loss: 174.28615379333496\n",
      "Epoch 322, loss: 174.35003280639648\n",
      "Epoch 323, loss: 174.42659950256348\n",
      "Epoch 324, loss: 174.64583015441895\n",
      "Epoch 325, loss: 174.70232772827148\n",
      "Epoch 326, loss: 175.48858070373535\n",
      "Epoch 327, loss: 176.71508407592773\n",
      "Epoch 328, loss: 179.14259910583496\n",
      "Epoch 329, loss: 187.34185028076172\n",
      "Epoch 330, loss: 178.03297424316406\n",
      "Epoch 331, loss: 183.68198013305664\n",
      "Epoch 332, loss: 181.2612533569336\n",
      "Epoch 333, loss: 178.97599029541016\n",
      "Epoch 334, loss: 181.11676025390625\n",
      "Epoch 335, loss: 215.15336227416992\n",
      "Epoch 336, loss: 252.08522033691406\n",
      "Epoch 337, loss: 284.73168182373047\n",
      "Epoch 338, loss: 212.81729125976562\n",
      "Epoch 339, loss: 239.61441040039062\n",
      "Epoch 340, loss: 200.23041534423828\n",
      "Epoch 341, loss: 289.57682037353516\n",
      "Epoch 342, loss: 356.6740493774414\n",
      "Epoch 343, loss: 364.0071105957031\n",
      "Epoch 344, loss: 403.1295623779297\n",
      "Epoch 345, loss: 568.4481048583984\n",
      "Epoch 346, loss: 442.6941223144531\n",
      "Epoch 347, loss: 262.4029769897461\n",
      "Epoch 348, loss: 336.22086334228516\n",
      "Epoch 349, loss: 390.8721160888672\n",
      "Epoch 350, loss: 271.24837493896484\n",
      "Epoch 351, loss: 294.27734375\n",
      "Epoch 352, loss: 238.49817657470703\n",
      "Epoch 353, loss: 252.66306686401367\n",
      "Epoch 354, loss: 241.84278106689453\n",
      "Epoch 355, loss: 194.9447898864746\n",
      "Epoch 356, loss: 227.86309051513672\n",
      "Epoch 357, loss: 196.12368774414062\n",
      "Epoch 358, loss: 201.42366409301758\n",
      "Epoch 359, loss: 208.92570114135742\n",
      "Epoch 360, loss: 186.1396598815918\n",
      "Epoch 361, loss: 196.77802658081055\n",
      "Epoch 362, loss: 190.45350646972656\n",
      "Epoch 363, loss: 185.14764404296875\n",
      "Epoch 364, loss: 188.63755416870117\n",
      "Epoch 365, loss: 182.28290939331055\n",
      "Epoch 366, loss: 183.67425537109375\n",
      "Epoch 367, loss: 182.76922988891602\n",
      "Epoch 368, loss: 179.72153091430664\n",
      "Epoch 369, loss: 181.77207946777344\n",
      "Epoch 370, loss: 180.19781112670898\n",
      "Epoch 371, loss: 179.66826248168945\n",
      "Epoch 372, loss: 180.11334991455078\n",
      "Epoch 373, loss: 178.88801956176758\n",
      "Epoch 374, loss: 179.0352325439453\n",
      "Epoch 375, loss: 178.9599723815918\n",
      "Epoch 376, loss: 178.4631748199463\n",
      "Epoch 377, loss: 178.4730567932129\n",
      "Epoch 378, loss: 178.20379447937012\n",
      "Epoch 379, loss: 178.02967071533203\n",
      "Epoch 380, loss: 178.02939987182617\n",
      "Epoch 381, loss: 177.77662658691406\n",
      "Epoch 382, loss: 177.74121856689453\n",
      "Epoch 383, loss: 177.72557258605957\n",
      "Epoch 384, loss: 177.54229927062988\n",
      "Epoch 385, loss: 177.51959991455078\n",
      "Epoch 386, loss: 177.47608375549316\n",
      "Epoch 387, loss: 177.34016036987305\n",
      "Epoch 388, loss: 177.3717441558838\n",
      "Epoch 389, loss: 177.29828453063965\n",
      "Epoch 390, loss: 177.1836700439453\n",
      "Epoch 391, loss: 177.16141510009766\n",
      "Epoch 392, loss: 177.0984344482422\n",
      "Epoch 393, loss: 177.0316390991211\n",
      "Epoch 394, loss: 177.01827430725098\n",
      "Epoch 395, loss: 176.9614963531494\n",
      "Epoch 396, loss: 176.8917293548584\n",
      "Epoch 397, loss: 176.84783554077148\n",
      "Epoch 398, loss: 176.82600212097168\n",
      "Epoch 399, loss: 176.75709342956543\n",
      "Epoch 400, loss: 176.7366542816162\n",
      "Epoch 401, loss: 176.6832618713379\n",
      "Epoch 402, loss: 176.6373462677002\n",
      "Epoch 403, loss: 176.60857391357422\n",
      "Epoch 404, loss: 176.5852508544922\n",
      "Epoch 405, loss: 176.54632949829102\n",
      "Epoch 406, loss: 176.50035858154297\n",
      "Epoch 407, loss: 176.45864868164062\n",
      "Epoch 408, loss: 176.40540885925293\n",
      "Epoch 409, loss: 176.38755798339844\n",
      "Epoch 410, loss: 176.34278869628906\n",
      "Epoch 411, loss: 176.30308151245117\n",
      "Epoch 412, loss: 176.26611518859863\n",
      "Epoch 413, loss: 176.23497009277344\n",
      "Epoch 414, loss: 176.21929550170898\n",
      "Epoch 415, loss: 176.18427848815918\n",
      "Epoch 416, loss: 176.14134216308594\n",
      "Epoch 417, loss: 176.09557151794434\n",
      "Epoch 418, loss: 176.07839584350586\n",
      "Epoch 419, loss: 176.0345916748047\n",
      "Epoch 420, loss: 176.00057983398438\n",
      "Epoch 421, loss: 175.9546127319336\n",
      "Epoch 422, loss: 175.92148971557617\n",
      "Epoch 423, loss: 175.91674995422363\n",
      "Epoch 424, loss: 175.88689613342285\n",
      "Epoch 425, loss: 175.84647750854492\n",
      "Epoch 426, loss: 175.80426788330078\n",
      "Epoch 427, loss: 175.78682136535645\n",
      "Epoch 428, loss: 175.74425888061523\n",
      "Epoch 429, loss: 175.71830940246582\n",
      "Epoch 430, loss: 175.67794609069824\n",
      "Epoch 431, loss: 175.6484146118164\n",
      "Epoch 432, loss: 175.62503242492676\n",
      "Epoch 433, loss: 175.60608863830566\n",
      "Epoch 434, loss: 175.57328987121582\n",
      "Epoch 435, loss: 175.53197479248047\n",
      "Epoch 436, loss: 175.5098705291748\n",
      "Epoch 437, loss: 175.47402572631836\n",
      "Epoch 438, loss: 175.4432601928711\n",
      "Epoch 439, loss: 175.40771484375\n",
      "Epoch 440, loss: 175.38165473937988\n",
      "Epoch 441, loss: 175.35993003845215\n",
      "Epoch 442, loss: 175.33588218688965\n",
      "Epoch 443, loss: 175.31691551208496\n",
      "Epoch 444, loss: 175.28345489501953\n",
      "Epoch 445, loss: 175.24184799194336\n",
      "Epoch 446, loss: 175.2291717529297\n",
      "Epoch 447, loss: 175.19413566589355\n",
      "Epoch 448, loss: 175.1642360687256\n",
      "Epoch 449, loss: 175.1328830718994\n",
      "Epoch 450, loss: 175.10336303710938\n",
      "Epoch 451, loss: 175.08048248291016\n",
      "Epoch 452, loss: 175.06001091003418\n",
      "Epoch 453, loss: 175.03493309020996\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-443-3efacf616be2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqerror_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "min_val = min_norm\n",
    "max_val = max_norm\n",
    "for epoch in range(num_epochs):\n",
    "    loss_total = 0.\n",
    "\n",
    "    for batch_idx, data_batch in enumerate(train_data_loader):\n",
    "        tables, predicates, joins, targets, table_masks, predicate_masks, join_masks = data_batch\n",
    "        tables, predicates, joins, targets = Variable(tables), Variable(predicates), Variable(joins), Variable(targets)\n",
    "        table_masks, predicate_masks, join_masks = Variable(table_masks), Variable(predicate_masks), Variable(join_masks)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(tables, predicates, joins, table_masks, predicate_masks, join_masks)\n",
    "        loss = qerror_loss(outputs, targets.float(), min_val, max_val)\n",
    "        loss_total += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch {}, loss: {}\".format(epoch, loss_total / len(train_data_loader)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def predict(model, data_loader):\n",
    "    preds = []\n",
    "    t_total = 0.\n",
    "\n",
    "    model.eval()\n",
    "    for batch_idx, data_batch in enumerate(data_loader):\n",
    "        tables, predicates, joins, targets, tables_masks, predicate_masks, join_masks = data_batch\n",
    "        tables, predicates, joins, targets = Variable(tables), Variable(predicates), Variable(joins), Variable(targets)\n",
    "        tables_masks, predicate_masks, join_masks = Variable(tables_masks), Variable(predicate_masks), Variable(join_masks)\n",
    "\n",
    "        t = time.time()\n",
    "        outputs = model(tables, predicates, joins, tables_masks, predicate_masks, join_masks)\n",
    "        t_total += time.time() - t\n",
    "\n",
    "        for i in range(outputs.data.shape[0]):\n",
    "            preds.append(outputs.data[i])\n",
    "\n",
    "    return preds, t_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time per training sample: 0.29283513174732023\n",
      "Prediction time per validation sample: 0.24759284849089336\n",
      "\n",
      "Q-Error training set:\n",
      "Median: 1.1290806278513743\n",
      "90th percentile: 2.1267726898193358\n",
      "95th percentile: 3.3870184659957894\n",
      "99th percentile: 6.29696743965149\n",
      "Max: inf\n",
      "Mean: inf\n",
      "\n",
      "Q-Error validation set:\n",
      "Median: 1.1141048669815063\n",
      "90th percentile: 1.6991142034530637\n",
      "95th percentile: 2.2249915599822994\n",
      "99th percentile: 7.459270887374881\n",
      "Max: 13.154102325439453\n",
      "Mean: 1.45118680501046\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get final training and validation set predictions\n",
    "preds_train, t_total = predict(model, train_data_loader)\n",
    "print(\"Prediction time per training sample: {}\".format(t_total / len(labels_train) * 1000))\n",
    "\n",
    "preds_test, t_total = predict(model, test_data_loader)\n",
    "print(\"Prediction time per validation sample: {}\".format(t_total / len(labels_test) * 1000))\n",
    "\n",
    "# Unnormalize\n",
    "preds_train_unnorm = unnormalize_labels(preds_train, min_val, max_val)\n",
    "labels_train_unnorm = unnormalize_labels(labels_train, min_val, max_val)\n",
    "\n",
    "preds_test_unnorm = unnormalize_labels(preds_test, min_val, max_val)\n",
    "labels_test_unnorm = unnormalize_labels(labels_test, min_val, max_val)\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\nQ-Error training set:\")\n",
    "print_qerror(preds_train, labels_train)\n",
    "\n",
    "print(\"\\nQ-Error validation set:\")\n",
    "print_qerror(preds_test, labels_test)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def predict_and_compare(random_sample):\n",
    "    rsample_data = dataset.TensorDataset(\n",
    "        tables_tensor[random_sample:random_sample+1], \n",
    "        predicates_tensor[random_sample:random_sample+1], \n",
    "        joins_tensor[random_sample:random_sample+1], \n",
    "        target_tensor[random_sample:random_sample+1], \n",
    "        tables_mask_tensor[random_sample:random_sample+1],\n",
    "        predicates_mask_tensor[random_sample:random_sample+1], \n",
    "        joins_mask_tensor[random_sample:random_sample+1]\n",
    "    )\n",
    "    rsample_data_loader = DataLoader(rsample_data, batch_size=1)\n",
    "    rsample_pred, _ = predict(model, rsample_data_loader)\n",
    "\n",
    "    #print(rsample_pred[-1], target_tensor[random_sample])\n",
    "    rsample_pred = unnormalize_torch(rsample_pred[-1], min_val, max_val)\n",
    "    rsample_label = unnormalize_torch(target_tensor[random_sample], min_val, max_val)\n",
    "\n",
    "    diff = np.abs(float(rsample_pred[-1] - rsample_label))\n",
    "\n",
    "    return diff, f'prediction :{float(rsample_pred)}, label:{rsample_label} , diff: {diff}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1225"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.367592080296302\n"
     ]
    }
   ],
   "source": [
    "diffs = []\n",
    "for i in range(0,len(target_tensor)):\n",
    "    d, _ = predict_and_compare(i)\n",
    "    diffs.append(d)\n",
    "    \n",
    "print(np.sum(diffs)/len(diffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.036812037229537964, 'prediction :0.17577865719795227, label:0.21259069442749023 , diff: 0.036812037229537964')\n",
      "(2.2051830291748047, 'prediction :21.41802406311035, label:23.623207092285156 , diff: 2.2051830291748047')\n",
      "(5.418582916259766, 'prediction :11.518852233886719, label:6.100269317626953 , diff: 5.418582916259766')\n",
      "(0.5922247171401978, 'prediction :0.7318159341812134, label:0.139591246843338 , diff: 0.5922247171401978')\n",
      "(3.8741626739501953, 'prediction :5.837990760803223, label:1.963828206062317 , diff: 3.8741626739501953')\n",
      "(0.20415246486663818, 'prediction :0.3751606047153473, label:0.1710081398487091 , diff: 0.20415246486663818')\n",
      "(1.3023731708526611, 'prediction :3.6406211853027344, label:2.3382480144500732 , diff: 1.3023731708526611')\n",
      "(0.7113326787948608, 'prediction :0.8549999594688416, label:0.14366725087165833 , diff: 0.7113326787948608')\n",
      "(2.9207940101623535, 'prediction :2.971230983734131, label:0.05043698847293854 , diff: 2.9207940101623535')\n",
      "(17.863250732421875, 'prediction :44.94786834716797, label:27.084617614746094 , diff: 17.863250732421875')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for i in random.sample(range(0,len(target_tensor)),10):\n",
    "    print(predict_and_compare(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
